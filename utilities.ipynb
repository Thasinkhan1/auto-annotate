{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kaggle as kg\n",
    "import os\n",
    "import cv2\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import ast\n",
    "from keras.models import Model\n",
    "from keras.applications import VGG16\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Conv2D\n",
    "from xml.etree import ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_boxes(image_size,grids_size,aspect_ratios):\n",
    "\n",
    "    image_width, image_height, _ = image_size\n",
    "\n",
    "    grid_width = image_width//grids_size[0]\n",
    "    grid_height = image_height//grids_size[1]\n",
    "\n",
    "    grid_center_x_start = grid_width//2\n",
    "    grid_center_x_end = (grids_size[0] - 0.5)*grid_width \n",
    "\n",
    "    grid_center_x = np.linspace(grid_center_x_start,grid_center_x_end,grids_size[0])\n",
    "\n",
    "    grid_center_y_start = grid_height//2\n",
    "    grid_center_y_end = (grids_size[1] - 0.5)*grid_height\n",
    "\n",
    "    grid_center_y = np.linspace(grid_center_y_start,grid_center_y_end,grids_size[1])\n",
    "\n",
    "    grid_center_x_mesh, grid_center_y_mesh = np.meshgrid(grid_center_x,grid_center_y)\n",
    "\n",
    "    grid_center_x_mesh = np.expand_dims(grid_center_x_mesh,-1)\n",
    "    grid_center_y_mesh = np.expand_dims(grid_center_y_mesh,-1)\n",
    "\n",
    "    anchor_boxes_no = len(aspect_ratios)\n",
    "\n",
    "    anchor_boxes_tensor = np.zeros((grids_size[0],grids_size[1],anchor_boxes_no,4))\n",
    "\n",
    "    anchor_boxes_tensor[...,0] = np.tile(grid_center_x_mesh,(1,1,anchor_boxes_no))\n",
    "    anchor_boxes_tensor[...,1] = np.tile(grid_center_y_mesh,(1,1,anchor_boxes_no))\n",
    "\n",
    "    anchor_box_width_height = list()\n",
    "\n",
    "    for aspect_ratio in aspect_ratios:\n",
    "\n",
    "        anchor_box_width_height.append((grid_width*np.sqrt(aspect_ratio),\n",
    "                                        grid_height/np.sqrt(aspect_ratio)))\n",
    "        \n",
    "    anchor_box_width_height = np.array(anchor_box_width_height)\n",
    "\n",
    "    anchor_boxes_tensor[...,2] = anchor_box_width_height[:,0]\n",
    "    anchor_boxes_tensor[...,3] = anchor_box_width_height[:,1]\n",
    "\n",
    "    return anchor_boxes_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid2minmax(anchor_boxes_centroid_tensor):\n",
    "\n",
    "    anchor_boxes_minmax_tensor = np.copy(anchor_boxes_centroid_tensor)\n",
    "\n",
    "    anchor_boxes_minmax_tensor[...,0] = anchor_boxes_minmax_tensor[...,0] - (anchor_boxes_minmax_tensor[...,2]//2)\n",
    "    anchor_boxes_minmax_tensor[...,1] = anchor_boxes_minmax_tensor[...,1] - (anchor_boxes_minmax_tensor[...,3]//2)\n",
    "    anchor_boxes_minmax_tensor[...,2] = anchor_boxes_minmax_tensor[...,0] + (anchor_boxes_minmax_tensor[...,2]//2)\n",
    "    anchor_boxes_minmax_tensor[...,3] = anchor_boxes_minmax_tensor[...,1] + (anchor_boxes_minmax_tensor[...,3]//2)\n",
    "\n",
    "    return anchor_boxes_minmax_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IoU(anchor_boxes_tensor,image_gt_bbox_coords):\n",
    "\n",
    "    IoU_tensor = np.zeros((len(image_gt_bbox_coords),anchor_boxes_tensor.shape[0],anchor_boxes_tensor.shape[1],anchor_boxes_tensor.shape[2]))\n",
    "    anchor_boxes_minmax_tensor = centroid2minmax(anchor_boxes_tensor)\n",
    "\n",
    "    gt_bboxes_mask = np.zeros((len(image_gt_bbox_coords),anchor_boxes_tensor.shape[0],anchor_boxes_tensor.shape[1],anchor_boxes_tensor.shape[2]))\n",
    "\n",
    "    for i in range(len(image_gt_bbox_coords)):\n",
    "\n",
    "        centroid_x_condition = (anchor_boxes_minmax_tensor[:,:,0,0]<(image_gt_bbox_coords[i,0]+image_gt_bbox_coords[i,2]//2)) & (anchor_boxes_minmax_tensor[:,:,0,2]>(image_gt_bbox_coords[i,0]+image_gt_bbox_coords[i,2]//2))\n",
    "        centroid_y_condition = (anchor_boxes_minmax_tensor[:,:,0,1]<(image_gt_bbox_coords[i,1]+image_gt_bbox_coords[i,3]//2)) & (anchor_boxes_minmax_tensor[:,:,0,3]>(image_gt_bbox_coords[i,1]+image_gt_bbox_coords[i,3]//2))\n",
    "\n",
    "        idxes = np.argwhere((centroid_x_condition & centroid_y_condition))\n",
    "\n",
    "        gt_bboxes_mask[i,idxes,:] = 1.0\n",
    "\n",
    "        for j in range(anchor_boxes_tensor.shape[2]):\n",
    "\n",
    "            xmin_intersection = np.max(image_gt_bbox_coords[i][0],anchor_boxes_minmax_tensor[:,:,j,0])\n",
    "            ymin_intersection = np.max(image_gt_bbox_coords[i][1],anchor_boxes_minmax_tensor[:,:,j,1])\n",
    "\n",
    "            xmax_intersection = np.min(image_gt_bbox_coords[i][0]+image_gt_bbox_coords[i][2],anchor_boxes_minmax_tensor[:,:,j,2])\n",
    "            ymax_intersection = np.min(image_gt_bbox_coords[i][1]+image_gt_bbox_coords[i][3],anchor_boxes_minmax_tensor[:,:,j,3])\n",
    "\n",
    "            intersection_width = np.max(0,(xmax_intersection - xmin_intersection))\n",
    "            intersection_height = np.max(0,(ymax_intersection - ymin_intersection))\n",
    "\n",
    "            intersection_area = intersection_width * intersection_height\n",
    "\n",
    "            image_gt_bbox_area = image_gt_bbox_coords[i][2] * image_gt_bbox_coords[i][3]\n",
    "            anchor_boxes_width = (anchor_boxes_minmax_tensor[:,:,j,2] - anchor_boxes_minmax_tensor[:,:,j,0])\n",
    "            anchor_boxes_height = (anchor_boxes_minmax_tensor[:,:,j,3] - anchor_boxes_minmax_tensor[:,:,j,1])\n",
    "\n",
    "            union_area = ((anchor_boxes_width * anchor_boxes_height) + image_gt_bbox_area) - intersection_area\n",
    "\n",
    "            IoU_tensor[i,:,:,j] = intersection_area/union_area\n",
    "    \n",
    "    confidence_score = IoU_tensor * gt_bboxes_mask\n",
    "\n",
    "    return gt_bboxes_mask, IoU_tensor, confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bbox_coords(image_size, image_gt_bbox_coords, gt_bboxes_mask, anchor_boxes_tensor):\n",
    "\n",
    "    image_width, image_height, _ = image_size\n",
    "    normalized_image_gt_bbox_coords = np.zeros_like(anchor_boxes_tensor)\n",
    "    \n",
    "    for i in range(len(image_gt_bbox_coords)):\n",
    "\n",
    "        idx = np.argwhere(gt_bboxes_mask[i,:,:,0] == 1.0)\n",
    "        normalized_image_gt_bbox_coords[idx,:,0] = (image_gt_bbox_coords[i][0] + (image_gt_bbox_coords[i][2]//2))/(image_gt_bbox_coords[i][0] + image_gt_bbox_coords[i][2])\n",
    "        normalized_image_gt_bbox_coords[idx,:,1] = (image_gt_bbox_coords[i][1] + (image_gt_bbox_coords[i][3]//2))/(image_gt_bbox_coords[i][1] + image_gt_bbox_coords[i][3])\n",
    "        normalized_image_gt_bbox_coords[idx,:,2] = image_gt_bbox_coords[i][2]//image_width\n",
    "        normalized_image_gt_bbox_coords[idx,:,3] = image_gt_bbox_coords[i][3]//image_height\n",
    "\n",
    "    return normalized_image_gt_bbox_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gt_labels_tensor(normalized_image_gt_bbox_coords, IoU_tensor, gt_bboxes_mask, image_cls_labels):\n",
    "\n",
    "    cls_probabilities_tensor = np.zeros((normalized_image_gt_bbox_coords.shape[0],normalized_image_gt_bbox_coords.shape[1],80))\n",
    "\n",
    "    for i in range(gt_bboxes_mask.shape[0]):\n",
    "\n",
    "        idx = np.argwhere(gt_bboxes_mask[i,:,:,0] == 1.0)\n",
    "        cls_probabilities_tensor[idx,:] = np.eye(80,80)[image_cls_labels[i]]\n",
    "\n",
    "    gt_labels_tensor = np.copy(normalized_image_gt_bbox_coords)\n",
    "    modified_iou_tensor = IoU_tensor[0,:,:,:]\n",
    "    modified_iou_tensor = np.expand_dims(modified_iou_tensor,-1)\n",
    "    gt_labels_tensor = np.concatenate((gt_labels_tensor,modified_iou_tensor),axis=3)\n",
    "    gt_labels_tensor = gt_labels_tensor.reshape(gt_labels_tensor.shape[0],gt_labels_tensor.shape[1],gt_labels_tensor.shape[2]*gt_labels_tensor.shape [3])\n",
    "\n",
    "    gt_labels_tensor = np.concatenate((gt_labels_tensor,cls_probabilities_tensor),axis=2)\n",
    "    \n",
    "    return gt_labels_tensor  #(7,7,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_cnn():\n",
    "\n",
    "    vgg16 = VGG16(include_top=False,input_shape=(640,480,3),weights=\"imagenet\",pooling=None)\n",
    "    vgg16.trainable = False\n",
    "    input_to_vgg16 = vgg16.input\n",
    "    vgg16_output = Conv2D(filters=90,kernel_size=(14,9))(vgg16.layers[-1].output)\n",
    "\n",
    "    return Model(inputs=[input_to_vgg16],outputs=[vgg16_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 640, 480, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 640, 480, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 640, 480, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 320, 240, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 320, 240, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 320, 240, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 160, 120, 128)     0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 160, 120, 256)     295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 160, 120, 256)     590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 160, 120, 256)     590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 80, 60, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 80, 60, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 80, 60, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 80, 60, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 40, 30, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 40, 30, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 40, 30, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 40, 30, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 20, 15, 512)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 90)          5806170   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20520858 (78.28 MB)\n",
      "Trainable params: 5806170 (22.15 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = multiclass_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_df(imgs_base_path,annotations_base_path):\n",
    "\n",
    "    img_complete_paths = list()\n",
    "    img_class_labels = list()\n",
    "    img_gt_bbox_coords = list()\n",
    "\n",
    "    for single_img_complete_path in pathlib.Path(imgs_base_path).glob(\"*\"):\n",
    "\n",
    "        img_path = str(single_img_complete_path)\n",
    "        img_label_path = os.path.join(annotations_base_path,str(single_img_complete_path).split(\"/\")[-1].split(\".\")[0]+\".xml\")\n",
    "\n",
    "        class_gt_labels_list = list()\n",
    "        gt_bbox_coords_list = list()\n",
    "\n",
    "        tree = ET.parse(img_label_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for member in root.findall(\"object\"):\n",
    "            \"\"\"\n",
    "            for child in member:\n",
    "\n",
    "                if child.tag == \"name\":\n",
    "                    class_gt_labels_list.append(child.text)\n",
    "\n",
    "                if child.tag == \"bndbox\":\n",
    "                    xmin = float(child[0].text)\n",
    "                    ymin = float(child[1].text)\n",
    "                    xmax = float(child[2].text)\n",
    "                    ymax = float(child[3].text)\n",
    "            \"\"\"\n",
    "            class_gt_labels_list.append(member.find(\"name\").text)\n",
    "            xmin = float(member.find(\"bndbox/xmin\").text)\n",
    "            ymin = float(member.find(\"bndbox/ymin\").text)\n",
    "            xmax = float(member.find(\"bndbox/xmax\").text)\n",
    "            ymax = float(member.find(\"bndbox/ymax\").text)\n",
    "            \n",
    "            bbox_width = xmax - xmin\n",
    "            bbox_height = ymax - ymin\n",
    "            \n",
    "\n",
    "            gt_bbox_coords_list.append([xmin,ymin,bbox_width,bbox_height])\n",
    "\n",
    "        img_complete_paths.append(str(single_img_complete_path))\n",
    "        img_class_labels.append(class_gt_labels_list)\n",
    "        img_gt_bbox_coords.append(gt_bbox_coords_list)\n",
    "\n",
    "    return pd.DataFrame(data={\"img_path\":img_complete_paths,\n",
    "                              \"img_gt_class_labels\":img_class_labels,\n",
    "                              \"img_gt_bbox_coords\":img_gt_bbox_coords})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = train_test_df(\"/home/thasin/class-projects/annotate/dataset/VOC2012_train_val/VOC2012_train_val/JPEGImages\",\"/home/thasin/class-projects/annotate/dataset/VOC2012_train_val/VOC2012_train_val/Annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17125, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "\n",
    "for img_labels in data_df.iloc[:,1]:\n",
    "    unique_labels = unique_labels.union(set(img_labels))\n",
    "\n",
    "unique_labels = list(unique_labels)\n",
    "#unique_labels.insert(0,\"background\")\n",
    "\n",
    "labels2idx = dict(zip(unique_labels,range(len(unique_labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2idx_mapping(img_labels):\n",
    "\n",
    "    return list(map(lambda x: labels2idx[x],img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"img_gt_class_labels\"] = data_df[\"img_gt_class_labels\"].apply(json.dumps)\n",
    "data_df[\"img_gt_bbox_coords\"] = data_df[\"img_gt_bbox_coords\"].apply(json.dumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_df\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels2idx_mapping\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/class-projects/annotate/.venv/lib/python3.8/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/class-projects/annotate/.venv/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/class-projects/annotate/.venv/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/class-projects/annotate/.venv/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m, in \u001b[0;36mlabels2idx_mapping\u001b[0;34m(img_labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlabels2idx_mapping\u001b[39m(img_labels):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels2idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m, in \u001b[0;36mlabels2idx_mapping.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlabels2idx_mapping\u001b[39m(img_labels):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mlabels2idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m,img_labels))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "data_df.iloc[:,1] = data_df.iloc[:,1].apply(labels2idx_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data_df.iloc[0:15000,:]\n",
    "cv_data = data_df.iloc[15000:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"img_gt_class_labels\"] = training_data[\"img_gt_class_labels\"].apply(ast.literal_eval)\n",
    "training_data[\"img_gt_bbox_coords\"] = training_data[\"img_gt_bbox_coords\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imread(training_data.iloc[0,0]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_boxes_tensor = anchor_boxes((640,480,3),(7,7),(1/2,2))\n",
    "anchor_boxes_minmax_tensor = centroid2minmax(anchor_boxes_tensor)\n",
    "\"\"\"\n",
    "gt_bboxes_mask,iou_tensor = compute_IoU(anchor_boxes_tensor,training_data.iloc[0,2])\n",
    "#normalized_gt_bbox_coords = normalize_bbox_coords((640,480,3),training_data.iloc[0,2],gt_bboxes_mask,anchor_boxes_tensor)\n",
    "#gt_labels_tensor = create_gt_labels_tensor(normalized_gt_bbox_coords,iou_tensor,gt_bboxes_mask,training_data.iloc[0,1],\n",
    "                                           20)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_generator(df,mb_size):\n",
    "\n",
    "    for i in range(df.shape[0]//mb_size):\n",
    "\n",
    "        X_train_mb = list()\n",
    "        Y_train_mb = list()\n",
    "        GT_mask_train_mb = list()\n",
    "\n",
    "        for j in range(0,mb_size):\n",
    "\n",
    "            df_mb = df.iloc[(i*mb_size)+j]\n",
    "            img_path = df_mb[\"img_path\"]\n",
    "\n",
    "            X_train_mb.append(cv2.resize(plt.imread(img_path),(640,480)))\n",
    "\n",
    "            gt_bboxes_mask, iou_tensor = compute_IoU(anchor_boxes_tensor,df_mb[\"img_gt_bbox_coords\"])\n",
    "            normalized_img_gt_bbox_coords = normalize_bbox_coords((640,480,3),df_mb[\"img_gt_bbox_coords\"],\n",
    "                                                                  gt_bboxes_mask,anchor_boxes_tensor)\n",
    "            Y_train, final_gt_bboxes_mask = create_gt_labels_tensor(normalized_img_gt_bbox_coords,iou_tensor,\n",
    "                                                                    gt_bboxes_mask,df_mb[\"img_gt_class_labels\"],20)\n",
    "            \n",
    "            Y_train_mb.append(Y_train)\n",
    "            GT_mask_train_mb.append(final_gt_bboxes_mask)\n",
    "            \n",
    "        yield np.array(X_train_mb), np.array(Y_train_mb), np.array(GT_mask_train_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_generator = custom_data_generator(training_data,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_fn(Y_true_mb,Y_pred_mb,GT_mask_train_mb,lambda_coord,lambda_noobj):\n",
    "\n",
    "    squared_error = (Y_true_mb - Y_pred_mb)**2\n",
    "\n",
    "    \"\"\"\n",
    "    squared_error_with_mask = GT_mask_train_mb * squared_error\n",
    "    squared_error_with_neg_mask = (1.0 - GT_mask_train_mb) * squared_error\n",
    "    \"\"\"\n",
    "\n",
    "    cx_cy_squared_error_tensor = np.concatenate((GT_mask_train_mb*squared_error[:,:,:,0:2],\n",
    "                                                 GT_mask_train_mb*squared_error[:,:,:,5:7]),axis=0)\n",
    "    \n",
    "    sqrt_squared_error = (np.sqrt(Y_true_mb) - np.sqrt(Y_pred_mb))**2\n",
    "\n",
    "    #sqrt_squared_error_with_mask = GT_mask_train_mb * sqrt_squared_error\n",
    "    \n",
    "    wh_sqrt_squared_error_tensor = np.concatenate((GT_mask_train_mb*sqrt_squared_error[:,:,:,2:4],\n",
    "                                                   GT_mask_train_mb*sqrt_squared_error[:,:,:,7:9]),axis=0)\n",
    "    \n",
    "    loss_fn_first_term = lambda_coord*np.sum(cx_cy_squared_error_tensor)\n",
    "    loss_fn_second_term = lambda_coord*np.sum(wh_sqrt_squared_error_tensor)\n",
    "\n",
    "    confidence_score_error_tensor = GT_mask_train_mb*np.concatenate((squared_error[:,:,:,4],\n",
    "                                                                     squared_error[:,:,:,9]),axis=0)\n",
    "    \n",
    "    loss_fn_third_term = np.sum(confidence_score_error_tensor)\n",
    "\n",
    "    confidence_score_noobj_error_tensor = (1.0 - GT_mask_train_mb)*np.concatenate((squared_error[:,:,:,4],\n",
    "                                                                                   squared_error[:,:,:,9]),axis=0)\n",
    "\n",
    "    loss_fn_forth_term = lambda_noobj*np.sum(confidence_score_noobj_error_tensor)\n",
    "\n",
    "    loss_fn_fifth_term = GT_mask_train_mb*np.sum(squared_error[:,:,:,10:])\n",
    "\n",
    "    overall_loss_fn = loss_fn_first_term + loss_fn_second_term + loss_fn_third_term +\\\n",
    "                        loss_fn_forth_term + loss_fn_fifth_term\n",
    "    \n",
    "    return overall_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "training_data_mb_size = 5\n",
    "testing_data_mb_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(X_train_mb,Y_true_train_mb):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "            \n",
    "        Y_pred_train_mb = model(X_train_mb, training=True)\n",
    "        training_loss = custom_loss_fn(Y_true_train_mb, Y_pred_train_mb)\n",
    "\n",
    "    grads = tape.gradient(training_loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    train_acc_metric.update_state(Y_true_train_mb,Y_pred_train_mb)\n",
    "\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def testing_forward_pass(X_test_mb,Y_true_test_mb):\n",
    "\n",
    "    Y_pred_test_mb = model(X_test_mb,training=False)\n",
    "    testing_loss = custom_loss_fn(Y_true_test_mb,Y_pred_test_mb)\n",
    "    test_acc_metric.update_state(Y_true_test_mb,Y_pred_test_mb)\n",
    "\n",
    "    return testing_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     training_data_generator \u001b[38;5;241m=\u001b[39m custom_data_generator(training_data,\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m time_step, (X_train_mb, cv_data)\u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_data_generator):\n\u001b[1;32m      9\u001b[0m         training_loss \u001b[38;5;241m=\u001b[39m training_step(X_train_mb,cv_data)\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (time_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[37], line 15\u001b[0m, in \u001b[0;36mcustom_data_generator\u001b[0;34m(df, mb_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m img_path \u001b[38;5;241m=\u001b[39m df_mb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m X_train_mb\u001b[38;5;241m.\u001b[39mappend(cv2\u001b[38;5;241m.\u001b[39mresize(plt\u001b[38;5;241m.\u001b[39mimread(img_path),(\u001b[38;5;241m480\u001b[39m,\u001b[38;5;241m640\u001b[39m))) \u001b[38;5;66;03m# error\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m gt_bboxes_mask, iou_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_IoU\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor_boxes_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg_gt_bbox_coords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m normalized_img_gt_bbox_coords \u001b[38;5;241m=\u001b[39m normalize_bbox_coords((\u001b[38;5;241m640\u001b[39m,\u001b[38;5;241m480\u001b[39m,\u001b[38;5;241m3\u001b[39m),df_mb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_gt_bbox_coords\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m                                                       gt_bboxes_mask,anchor_boxes_tensors)\n\u001b[1;32m     18\u001b[0m Y_train_mb\u001b[38;5;241m.\u001b[39mappend(create_gt_labels_tensor(normalized_img_gt_bbox_coords,iou_tensor,gt_bboxes_mask,\n\u001b[1;32m     19\u001b[0m                                           df_mb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_gt_class_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;241m20\u001b[39m))\n",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m, in \u001b[0;36mcompute_IoU\u001b[0;34m(anchor_boxes_tensor, image_gt_bbox_coords)\u001b[0m\n\u001b[1;32m      6\u001b[0m gt_bboxes_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(image_gt_bbox_coords),anchor_boxes_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],anchor_boxes_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],anchor_boxes_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(image_gt_bbox_coords)):\n\u001b[0;32m---> 10\u001b[0m     centroid_x_condition \u001b[38;5;241m=\u001b[39m (anchor_boxes_minmax_tensor[:,:,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m<\u001b[39m(\u001b[43mimage_gt_bbox_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m+\u001b[39mimage_gt_bbox_coords[i,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m&\u001b[39m (anchor_boxes_minmax_tensor[:,:,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m>\u001b[39m(image_gt_bbox_coords[i,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mimage_gt_bbox_coords[i,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     11\u001b[0m     centroid_y_condition \u001b[38;5;241m=\u001b[39m (anchor_boxes_minmax_tensor[:,:,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m<\u001b[39m(image_gt_bbox_coords[i,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mimage_gt_bbox_coords[i,\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m&\u001b[39m (anchor_boxes_minmax_tensor[:,:,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m>\u001b[39m(image_gt_bbox_coords[i,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mimage_gt_bbox_coords[i,\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     13\u001b[0m     idxes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margwhere((centroid_x_condition \u001b[38;5;241m&\u001b[39m centroid_y_condition))\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    training_data_generator = custom_data_generator(training_data,5)\n",
    "\n",
    "    for time_step, (X_train_mb, cv_data)in enumerate(training_data_generator):\n",
    "        training_loss = training_step(X_train_mb,cv_data)\n",
    "\n",
    "        if (time_step+1) % 10 == 0:\n",
    "            print(\"Epoch %d, Time Step %d, Training loss for one mini batch: %.4f\"\n",
    "            % (epoch+1, time_step+1, float(training_loss)))\n",
    "            \n",
    "    training_acc = train_acc_metric.result()    \n",
    "    print(\"Epoch %d, Training Accuracy: %.2f\" % (epoch+1,float(training_acc)))\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    testing_data_generator = custom_data_generator(cv_data,5)\n",
    "\n",
    "    for X_test_mb in testing_data_generator:\n",
    "        testing_loss = testing_forward_pass(X_test_mb,5)\n",
    "\n",
    "    print(\"\\nEpoch %d, Testing Loss for last mini batch: %.4f\" % (epoch+1,float(testing_loss)))\n",
    "    testing_acc = test_acc_metric.result()\n",
    "    print(\"Epoch %d, Testing Accuracy: %.2f\" % (epoch+1,float(testing_acc)))\n",
    "    test_acc_metric.reset_states()\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Number of batches per epoch\u001b[39;00m\n\u001b[1;32m     38\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# You can also use Adam, SGD, etc.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with the appropriate loss function for YOLO\u001b[39;00m\n\u001b[1;32m     41\u001b[0m train_model(model, training_data_generator, epochs, steps_per_epoch, optimizer, loss_fn)\n",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(y_train_mb, y_pred_mb)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(y_train_mb, y_pred_mb):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                                          \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred_mb\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/class-projects/annotate/.venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/class-projects/annotate/.venv/lib/python3.8/site-packages/keras/src/losses.py:2098\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(axis, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m   2094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2095\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`axis` must be of type `int`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2096\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: axis=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(axis)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2097\u001b[0m     )\n\u001b[0;32m-> 2098\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2099\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(y_true, y_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   2100\u001b[0m label_smoothing \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(label_smoothing, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example training function\n",
    "def train_model(model, data_generator, epochs, steps_per_epoch, optimizer, loss_fn):\n",
    "    # Compile the model with the given optimizer and loss function\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Reset metrics at the start of each epoch\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Loop over batches within an epoch\n",
    "        for step in range(steps_per_epoch):\n",
    "            # Get the next batch from the data generator\n",
    "            X_train_mb, Y_train_mb = next(data_generator)\n",
    "\n",
    "            # Train the model on the current batch\n",
    "            batch_loss = model.train_on_batch(X_train_mb, Y_train_mb)\n",
    "\n",
    "            # Accumulate the loss for the current epoch\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                # Print progress for every 10 steps\n",
    "                print(f\"Step {step}/{steps_per_epoch} - Batch Loss: {batch_loss:.4f}\")\n",
    "\n",
    "        # Calculate and print the average loss for the epoch\n",
    "        avg_epoch_loss = epoch_loss / steps_per_epoch\n",
    "        print(f\"Epoch {epoch + 1} - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have a defined model, data generator, optimizer, and loss function\n",
    "epochs = 10\n",
    "steps_per_epoch = 100  # Number of batches per epoch\n",
    "optimizer = 'adam'  # You can also use Adam, SGD, etc.\n",
    "loss_fn = loss_fn(cv_data,training_data)  # Replace with the appropriate loss function for YOLO\n",
    "\n",
    "train_model(model, training_data_generator, epochs, steps_per_epoch, optimizer, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
